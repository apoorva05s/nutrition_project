{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31869a3-2f08-4540-b26b-371fb0892228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading processed datasets...\n",
      "‚úÖ Loaded nutrition dataset: (20130, 9)\n",
      "‚úÖ Loaded environment dataset: (20130, 3)\n",
      "‚úÖ Columns after merge:\n",
      "['recipe_id', 'recipe_title', 'ingredient_text', 'energy_kcal_mean', 'protein_g_mean', 'fat_g_mean', 'carbs_g_mean', 'price_mean', 'Total_emissions_mean', 'Total_emissions', 'Land_use_change']\n",
      "‚úÖ All required columns found.\n",
      "‚úÖ Final dataset shape: (0, 8)\n",
      "‚úÖ Columns used: ['recipe_title', 'energy_kcal_mean', 'protein_g_mean', 'fat_g_mean', 'carbs_g_mean', 'price_mean', 'Total_emissions', 'Land_use_change']\n",
      "üíæ Saved final merged dataset ‚Üí D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\recipes_final_for_optimization.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# üìò 03_merge_for_optimization.ipynb\n",
    "# Purpose: Merge nutritional + environmental + cost metrics\n",
    "#          for Safety Layer & NSGA-II Optimization\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Define paths\n",
    "# ---------------------------------------------------------\n",
    "BASE_DIR = Path(\"D:/Complete_Data/ml_part_nutrition_project\")\n",
    "PROCESSED_DIR = BASE_DIR / \"processed_data\"\n",
    "\n",
    "recipes_nutr_path = PROCESSED_DIR / \"recipes_enriched.csv\"\n",
    "recipes_env_path = PROCESSED_DIR / \"recipes_with_env_metrics.csv\"\n",
    "final_save_path = PROCESSED_DIR / \"recipes_final_for_optimization.csv\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Load datasets\n",
    "# ---------------------------------------------------------\n",
    "print(\"üì• Loading processed datasets...\")\n",
    "recipes_nutr = pd.read_csv(recipes_nutr_path)\n",
    "recipes_env = pd.read_csv(recipes_env_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded nutrition dataset: {recipes_nutr.shape}\")\n",
    "print(f\"‚úÖ Loaded environment dataset: {recipes_env.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Force recipe_title to string before merging\n",
    "# ---------------------------------------------------------\n",
    "recipes_nutr[\"recipe_title\"] = recipes_nutr[\"recipe_title\"].astype(str).str.strip()\n",
    "recipes_env[\"recipe_title\"] = recipes_env[\"recipe_title\"].astype(str).str.strip()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Merge on recipe_title\n",
    "# ---------------------------------------------------------\n",
    "recipes_merged = pd.merge(recipes_nutr, recipes_env, on=\"recipe_title\", how=\"left\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Clean column names\n",
    "# ---------------------------------------------------------\n",
    "recipes_merged.columns = recipes_merged.columns.str.replace(\" \", \"_\").str.strip()\n",
    "\n",
    "print(\"‚úÖ Columns after merge:\")\n",
    "print(list(recipes_merged.columns))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Select key columns for optimization\n",
    "# ---------------------------------------------------------\n",
    "expected_cols = [\n",
    "    \"recipe_title\",\n",
    "    \"energy_kcal_mean\",\n",
    "    \"protein_g_mean\",\n",
    "    \"fat_g_mean\",\n",
    "    \"carbs_g_mean\",\n",
    "    \"price_mean\",\n",
    "    \"Total_emissions\",\n",
    "    \"Land_use_change\"\n",
    "]\n",
    "\n",
    "available_cols = [c for c in expected_cols if c in recipes_merged.columns]\n",
    "recipes_final = recipes_merged[available_cols].dropna().reset_index(drop=True)\n",
    "\n",
    "if len(available_cols) < 6:\n",
    "    print(\"‚ö†Ô∏è Warning: Some expected columns are missing. Available columns:\", available_cols)\n",
    "else:\n",
    "    print(\"‚úÖ All required columns found.\")\n",
    "\n",
    "print(f\"‚úÖ Final dataset shape: {recipes_final.shape}\")\n",
    "print(f\"‚úÖ Columns used: {available_cols}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7Ô∏è‚É£ Save merged dataset\n",
    "# ---------------------------------------------------------\n",
    "recipes_final.to_csv(final_save_path, index=False)\n",
    "print(f\"üíæ Saved final merged dataset ‚Üí {final_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f032f25-f132-4c9d-a369-92cb7abbfdbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recipes_enriched: (20130, 9) -> D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\recipes_enriched.csv\n",
      "Saved safety rules -> D:\\Complete_Data\\ml_part_nutrition_project\\models\\safety_rules.json\n",
      "Saved safe recipes -> D:\\Complete_Data\\ml_part_nutrition_project\\results\\safety_by_user\\user_vegan_nuts_diabetes_recipes_safe.csv  (count: 4183)\n",
      "Saved flagged recipes -> D:\\Complete_Data\\ml_part_nutrition_project\\results\\safety_by_user\\user_vegan_nuts_diabetes_recipes_flagged.csv (count: 20130)\n",
      "Saved safe recipes -> D:\\Complete_Data\\ml_part_nutrition_project\\results\\safety_by_user\\user_hypertension_warfarin_recipes_safe.csv  (count: 5836)\n",
      "Saved flagged recipes -> D:\\Complete_Data\\ml_part_nutrition_project\\results\\safety_by_user\\user_hypertension_warfarin_recipes_flagged.csv (count: 20130)\n",
      "Saved safe recipes -> D:\\Complete_Data\\ml_part_nutrition_project\\results\\safety_by_user\\global_default_recipes_safe.csv  (count: 20130)\n",
      "Saved flagged recipes -> D:\\Complete_Data\\ml_part_nutrition_project\\results\\safety_by_user\\global_default_recipes_flagged.csv (count: 20130)\n",
      "Saved global safe dataset -> D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\recipes_safety_filtered.csv (count: 20130)\n",
      "Trained ML model saved -> D:\\Complete_Data\\ml_part_nutrition_project\\models\\safety_layer_rf.joblib\n",
      "Vectorizer saved -> D:\\Complete_Data\\ml_part_nutrition_project\\models\\safety_tfidf_vectorizer.joblib\n",
      "ML eval: {'accuracy': 0.9408842523596622, 'precision': 0.9544764795144158, 'recall': 0.7514934289127837}\n",
      "Saved summary -> D:\\Complete_Data\\ml_part_nutrition_project\\results\\safety_layer_summary.json\n",
      "‚úÖ Safety layer pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 03_safety_layer_nsga2_prep.py\n",
    "# Dynamic Safety Layer ‚Üí rule-based + small ML fallback + saving\n",
    "# Author: Generated for Apoorva Sharma\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# 0Ô∏è‚É£ Paths & Setup\n",
    "# ----------------------------\n",
    "BASE_DIR = Path(\"D:/Complete_Data/ml_part_nutrition_project\")   # <-- update if necessary\n",
    "PROCESSED_DIR = BASE_DIR / \"processed_data\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input file(s) expected\n",
    "RECIPES_ENRICHED = PROCESSED_DIR / \"recipes_enriched.csv\"         # preferred\n",
    "RECIPES_FALLBACK = PROCESSED_DIR / \"recipes_master_with_ingredients.csv\"  # fallback\n",
    "\n",
    "# Output files\n",
    "GLOBAL_SAFE_PATH = PROCESSED_DIR / \"recipes_safety_filtered.csv\"\n",
    "RULES_JSON_PATH = MODELS_DIR / \"safety_rules.json\"\n",
    "MODEL_PATH = MODELS_DIR / \"safety_layer_rf.joblib\"\n",
    "VECT_PATH = MODELS_DIR / \"safety_tfidf_vectorizer.joblib\"\n",
    "SUMMARY_JSON = RESULTS_DIR / \"safety_layer_summary.json\"\n",
    "PER_USER_DIR = RESULTS_DIR / \"safety_by_user\"\n",
    "PER_USER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1Ô∏è‚É£ Load dataset (robust)\n",
    "# ----------------------------\n",
    "def load_recipes():\n",
    "    if RECIPES_ENRICHED.exists():\n",
    "        df = pd.read_csv(RECIPES_ENRICHED)\n",
    "        print(f\"Loaded recipes_enriched: {df.shape} -> {RECIPES_ENRICHED}\")\n",
    "        return df\n",
    "    elif RECIPES_FALLBACK.exists():\n",
    "        df = pd.read_csv(RECIPES_FALLBACK)\n",
    "        print(f\"Loaded fallback recipes: {df.shape} -> {RECIPES_FALLBACK}\")\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Neither {RECIPES_ENRICHED} nor {RECIPES_FALLBACK} found. Place processed recipes CSV in {PROCESSED_DIR}\")\n",
    "\n",
    "recipes = load_recipes()\n",
    "\n",
    "# Ensure necessary columns exist; create safe defaults where needed\n",
    "def normalize_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Find ingredient text column or construct\n",
    "if \"ingredient_text\" not in recipes.columns:\n",
    "    # try common variants\n",
    "    candidates = [c for c in recipes.columns if \"ingredient\" in c.lower() or \"ing\" in c.lower()]\n",
    "    if candidates:\n",
    "        recipes[\"ingredient_text\"] = recipes[candidates[0]].astype(str)\n",
    "        print(f\"Using column '{candidates[0]}' as ingredient_text\")\n",
    "    else:\n",
    "        # attempt to combine any 'ingredient' lists in other formats\n",
    "        print(\"No ingredient text column found ‚Äî filling with empty strings (you should create recipe->ingredient mapping earlier).\")\n",
    "        recipes[\"ingredient_text\"] = \"\"\n",
    "\n",
    "recipes[\"ingredient_text_clean\"] = recipes[\"ingredient_text\"].astype(str).apply(normalize_text)\n",
    "recipes[\"title_clean\"] = recipes.get(\"title\", recipes.get(\"recipe_title\", \"\")).astype(str).apply(normalize_text)\n",
    "\n",
    "# ----------------------------\n",
    "# 2Ô∏è‚É£ Base knowledge dictionaries (you can extend)\n",
    "# ----------------------------\n",
    "ALLERGENS = {\n",
    "    \"nuts\": [\"almond\", \"walnut\", \"cashew\", \"peanut\", \"hazelnut\", \"pecan\", \"pistachio\", \"macadamia\"],\n",
    "    \"dairy\": [\"milk\", \"butter\", \"cheese\", \"cream\", \"yogurt\", \"ghee\", \"buttermilk\"],\n",
    "    \"gluten\": [\"wheat\", \"barley\", \"rye\", \"flour\", \"bread\", \"pasta\", \"semolina\"],\n",
    "    \"soy\": [\"soy\", \"soybean\", \"tofu\", \"soy sauce\", \"miso\"],\n",
    "    \"egg\": [\"egg\", \"mayonnaise\", \"meringue\"],\n",
    "    \"fish\": [\"fish\", \"salmon\", \"tuna\", \"cod\", \"anchovy\"],\n",
    "    \"shellfish\": [\"shrimp\", \"prawn\", \"crab\", \"lobster\", \"clam\", \"oyster\"],\n",
    "    \"sesame\": [\"sesame\", \"tahini\", \"sesame oil\"],\n",
    "    \"mustard\": [\"mustard\", \"mustard seed\", \"mustard powder\"]\n",
    "}\n",
    "\n",
    "DIETS = {\n",
    "    \"vegan\": [\"milk\", \"cheese\", \"butter\", \"meat\", \"egg\", \"fish\", \"honey\", \"yogurt\", \"gelatin\"],\n",
    "    \"vegetarian\": [\"meat\", \"pork\", \"beef\", \"chicken\"],\n",
    "    \"keto\": [\"rice\", \"bread\", \"sugar\", \"pasta\", \"potato\", \"banana\", \"corn\"],\n",
    "    \"paleo\": [\"processed\", \"bread\", \"pasta\", \"cereal\", \"legumes\"],\n",
    "    \"low_fodmap\": [\"garlic\", \"onion\", \"beans\", \"lentils\", \"apple\", \"pear\", \"mushroom\"]\n",
    "}\n",
    "\n",
    "HEALTH_RESTRICTIONS = {\n",
    "    \"diabetes\": [\"sugar\", \"honey\", \"syrup\", \"soda\", \"sweet\", \"candy\", \"cake\", \"cookie\", \"pastry\", \"white rice\", \"white bread\", \"refined flour\", \"white pasta\", \"dessert\", \"jam\", \"jelly\"],\n",
    "    \"hypertension\": [\"salt\", \"sodium\", \"soy sauce\", \"pickles\", \"canned\", \"processed meat\", \"ham\", \"bacon\", \"salami\", \"sausage\", \"instant noodles\", \"bouillon\"],\n",
    "    \"celiac\": [\"wheat\", \"barley\", \"rye\", \"malt\", \"triticale\", \"bulgur\", \"semolina\", \"farina\", \"graham\", \"spelt\", \"kamut\", \"durum\", \"flour\"],\n",
    "    \"pcos\": [\"sugar\", \"white bread\", \"fried\", \"processed snack\", \"soda\", \"refined carbs\", \"trans fat\", \"fast food\"],\n",
    "    \"kidney_disease\": [\"potassium\", \"banana\", \"avocado\", \"potato\", \"tomato\", \"spinach\", \"orange\", \"orange juice\", \"dried fruit\"],\n",
    "    \"gout\": [\"red meat\", \"organ meat\", \"liver\", \"kidney\", \"anchovy\", \"sardine\", \"mackerel\", \"tuna\", \"beer\"],\n",
    "    \"thyroid\": [\"soy\", \"broccoli\", \"cabbage\", \"cauliflower\", \"kale\", \"turnip\", \"cassava\"],\n",
    "    \"gerd\": [\"chocolate\", \"mint\", \"fried\", \"spicy\", \"tomato\", \"citrus\", \"onion\", \"garlic\", \"coffee\", \"alcohol\"]\n",
    "}\n",
    "\n",
    "MEDICATION_RESTRICTIONS = {\n",
    "    \"warfarin\": [\"kale\", \"spinach\", \"broccoli\", \"collard\", \"parsley\", \"cabbage\"],  # vitamin K\n",
    "    \"metformin\": [\"alcohol\"],  # simplified example\n",
    "    # add others as required\n",
    "}\n",
    "\n",
    "# Save rules JSON (human-readable)\n",
    "rules_to_save = {\n",
    "    \"ALLERGENS\": ALLERGENS,\n",
    "    \"DIETS\": DIETS,\n",
    "    \"HEALTH_RESTRICTIONS\": HEALTH_RESTRICTIONS,\n",
    "    \"MEDICATION_RESTRICTIONS\": MEDICATION_RESTRICTIONS\n",
    "}\n",
    "with open(RULES_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rules_to_save, f, indent=2)\n",
    "print(f\"Saved safety rules -> {RULES_JSON_PATH}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3Ô∏è‚É£ Utility functions for checking\n",
    "# ----------------------------\n",
    "def text_contains_any(text: str, keywords: list) -> bool:\n",
    "    if not keywords or not isinstance(text, str):\n",
    "        return False\n",
    "    t = normalize_text(text)\n",
    "    for kw in keywords:\n",
    "        k = kw.replace(\"_\", \" \").lower()\n",
    "        if k in t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def violates_allergen(text: str, user_allergies: list) -> bool:\n",
    "    for allergen in (user_allergies or []):\n",
    "        words = ALLERGENS.get(allergen, [])\n",
    "        if text_contains_any(text, words):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def violates_diet(text: str, user_diet: str) -> bool:\n",
    "    if not user_diet:\n",
    "        return False\n",
    "    return text_contains_any(text, DIETS.get(user_diet, []))\n",
    "\n",
    "def violates_health(text: str, conditions: list) -> dict:\n",
    "    flags = {}\n",
    "    for cond in (conditions or []):\n",
    "        flags[cond] = text_contains_any(text, HEALTH_RESTRICTIONS.get(cond, []))\n",
    "    return flags\n",
    "\n",
    "def violates_medications(text: str, medications: list) -> dict:\n",
    "    flags = {}\n",
    "    for med in (medications or []):\n",
    "        flags[med] = text_contains_any(text, MEDICATION_RESTRICTIONS.get(med.lower(), []))\n",
    "    return flags\n",
    "\n",
    "# ----------------------------\n",
    "# 4Ô∏è‚É£ Main filtering function\n",
    "# ----------------------------\n",
    "def filter_recipes_for_user(recipes_df: pd.DataFrame,\n",
    "                            user_allergies=None,\n",
    "                            user_diet=None,\n",
    "                            user_conditions=None,\n",
    "                            user_medications=None,\n",
    "                            save_personalized=True,\n",
    "                            user_id=\"user_default\",\n",
    "                            verbose=True):\n",
    "    user_allergies = user_allergies or []\n",
    "    user_conditions = user_conditions or []\n",
    "    user_medications = user_medications or []\n",
    "    df = recipes_df.copy()\n",
    "\n",
    "    if \"ingredient_text_clean\" not in df.columns:\n",
    "        df[\"ingredient_text_clean\"] = df[\"ingredient_text\"].fillna(\"\").astype(str).apply(normalize_text)\n",
    "\n",
    "    # Compute flags\n",
    "    df[\"contains_allergen\"] = df[\"ingredient_text_clean\"].apply(lambda x: violates_allergen(x, user_allergies))\n",
    "    df[\"violates_diet\"] = df[\"ingredient_text_clean\"].apply(lambda x: violates_diet(x, user_diet))\n",
    "\n",
    "    # conditions\n",
    "    for cond in user_conditions:\n",
    "        col = f\"violates_{cond}\"\n",
    "        kws = HEALTH_RESTRICTIONS.get(cond, [])\n",
    "        df[col] = df[\"ingredient_text_clean\"].apply(lambda x, kws=kws: text_contains_any(x, kws))\n",
    "\n",
    "    # medications\n",
    "    for med in user_medications:\n",
    "        col = f\"violates_med_{med.lower()}\"\n",
    "        kws = MEDICATION_RESTRICTIONS.get(med.lower(), [])\n",
    "        df[col] = df[\"ingredient_text_clean\"].apply(lambda x, kws=kws: text_contains_any(x, kws))\n",
    "\n",
    "    # combined flag list\n",
    "    extra_flags = [f\"violates_{c}\" for c in user_conditions] + [f\"violates_med_{m.lower()}\" for m in user_medications]\n",
    "    flag_cols = [\"contains_allergen\", \"violates_diet\"] + extra_flags\n",
    "    for c in flag_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = False\n",
    "\n",
    "    df[\"is_safe_for_user\"] = ~df[flag_cols].any(axis=1)\n",
    "    safe_df = df[df[\"is_safe_for_user\"]].reset_index(drop=True)\n",
    "    unsafe_df = df[~df[\"is_safe_for_user\"]].reset_index(drop=True)\n",
    "\n",
    "    if save_personalized:\n",
    "        out_path = PER_USER_DIR / f\"{user_id}_recipes_safe.csv\"\n",
    "        safe_df.to_csv(out_path, index=False)\n",
    "        # also save full flagged dataset\n",
    "        full_path = PER_USER_DIR / f\"{user_id}_recipes_flagged.csv\"\n",
    "        df.to_csv(full_path, index=False)\n",
    "        if verbose:\n",
    "            print(f\"Saved safe recipes -> {out_path}  (count: {len(safe_df)})\")\n",
    "            print(f\"Saved flagged recipes -> {full_path} (count: {len(df)})\")\n",
    "\n",
    "    return safe_df, df\n",
    "\n",
    "# ----------------------------\n",
    "# 5Ô∏è‚É£ Example user tests (and save global default)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example users (change as needed)\n",
    "    users = [\n",
    "        {\n",
    "            \"user_id\": \"user_vegan_nuts_diabetes\",\n",
    "            \"allergies\": [\"nuts\", \"gluten\"],\n",
    "            \"diet\": \"vegan\",\n",
    "            \"conditions\": [\"diabetes\"],\n",
    "            \"medications\": []\n",
    "        },\n",
    "        {\n",
    "            \"user_id\": \"user_hypertension_warfarin\",\n",
    "            \"allergies\": [],\n",
    "            \"diet\": None,\n",
    "            \"conditions\": [\"hypertension\"],\n",
    "            \"medications\": [\"warfarin\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    summary = {\"global\": {}}\n",
    "    for u in users:\n",
    "        safe, full = filter_recipes_for_user(\n",
    "            recipes,\n",
    "            user_allergies=u[\"allergies\"],\n",
    "            user_diet=u[\"diet\"],\n",
    "            user_conditions=u[\"conditions\"],\n",
    "            user_medications=u[\"medications\"],\n",
    "            save_personalized=True,\n",
    "            user_id=u[\"user_id\"],\n",
    "            verbose=True\n",
    "        )\n",
    "        summary[u[\"user_id\"]] = {\n",
    "            \"safe_count\": int(len(safe)),\n",
    "            \"total_recipes\": int(len(recipes))\n",
    "        }\n",
    "\n",
    "    # Save a global default (no restrictions)\n",
    "    global_safe, global_full = filter_recipes_for_user(recipes,\n",
    "                                                       user_allergies=[],\n",
    "                                                       user_diet=None,\n",
    "                                                       user_conditions=[],\n",
    "                                                       user_medications=[],\n",
    "                                                       save_personalized=True,\n",
    "                                                       user_id=\"global_default\",\n",
    "                                                       verbose=True)\n",
    "    global_safe.to_csv(GLOBAL_SAFE_PATH, index=False)\n",
    "    print(f\"Saved global safe dataset -> {GLOBAL_SAFE_PATH} (count: {len(global_safe)})\")\n",
    "    summary[\"global\"][\"safe_count\"] = int(len(global_safe))\n",
    "    summary[\"global\"][\"total_recipes\"] = int(len(recipes))\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6Ô∏è‚É£ Train a small ML model to predict is_safe_for_user (optional)\n",
    "    #    ‚Äî useful to speed up runtime checks (approximate rules)\n",
    "    # ----------------------------\n",
    "    # Build training dataset using the \"global\" rule labels (no user filters) vs an example user profile\n",
    "    # We'll create labels using the earlier function for one simulated user (e.g. user_vegan_nuts_diabetes)\n",
    "    train_user = users[0]\n",
    "    _, flagged = filter_recipes_for_user(recipes,\n",
    "                                        user_allergies=train_user[\"allergies\"],\n",
    "                                        user_diet=train_user[\"diet\"],\n",
    "                                        user_conditions=train_user[\"conditions\"],\n",
    "                                        user_medications=train_user[\"medications\"],\n",
    "                                        save_personalized=False,\n",
    "                                        user_id=\"train_user\",\n",
    "                                        verbose=False)\n",
    "    if flagged[\"is_safe_for_user\"].nunique() < 2:\n",
    "        print(\"Not enough label variety to train ML model (all safe or all unsafe). Skipping training.\")\n",
    "        summary[\"ml_model\"] = {\"trained\": False, \"reason\": \"not_enough_label_variability\"}\n",
    "    else:\n",
    "        # Vectorize ingredient text\n",
    "        vec = TfidfVectorizer(max_features=2000, stop_words=\"english\")\n",
    "        X = vec.fit_transform(flagged[\"ingredient_text_clean\"].astype(str)).toarray()\n",
    "        y = flagged[\"is_safe_for_user\"].astype(int).values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "        report = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n",
    "\n",
    "        # Save model + vectorizer\n",
    "        joblib.dump(clf, MODEL_PATH)\n",
    "        joblib.dump(vec, VECT_PATH)\n",
    "\n",
    "        # Save test results\n",
    "        ml_summary = {\n",
    "            \"trained\": True,\n",
    "            \"model_path\": str(MODEL_PATH),\n",
    "            \"vectorizer_path\": str(VECT_PATH),\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "        summary[\"ml_model\"] = ml_summary\n",
    "        print(f\"Trained ML model saved -> {MODEL_PATH}\")\n",
    "        print(f\"Vectorizer saved -> {VECT_PATH}\")\n",
    "        print(\"ML eval:\", {\"accuracy\": acc, \"precision\": prec, \"recall\": rec})\n",
    "\n",
    "    # Save summary\n",
    "    with open(SUMMARY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"Saved summary -> {SUMMARY_JSON}\")\n",
    "\n",
    "    print(\"‚úÖ Safety layer pipeline finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (new)",
   "language": "python",
   "name": "python3-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
