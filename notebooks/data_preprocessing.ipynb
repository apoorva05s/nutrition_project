{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d95fc0-d6ac-44fd-8611-03bac24bafbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in c:\\users\\apoorva\\anaconda3\\lib\\site-packages (3.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rapidfuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68f766f-a38f-41d8-b732-ef5fe9c07847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apoorva\\AppData\\Local\\Temp\\ipykernel_13188\\1396067817.py:44: DtypeWarning: Columns (4,5,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cost_data = pd.read_csv(COST_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded ingredients: (199106, 3)\n",
      "âœ… Loaded USDA food: (7793, 2)\n",
      "âœ… Loaded USDA nutrient: (644125, 11)\n",
      "âœ… Loaded environment: (43, 23)\n",
      "âœ… Loaded cost data: (172018, 14)\n",
      "âœ… Loaded recipes JSON: 20130 recipes\n",
      "ðŸ” Parsing recipe JSON...\n",
      "âœ… Extracted recipes: (20130, 3)\n",
      "âœ… USDA combined nutrient table: (7793, 7)\n",
      "âœ… Rapid matched USDA nutrient data\n",
      "âœ… Rapid matched environmental data\n",
      "ðŸ”¹ Cleaning cost data before averaging...\n",
      "âœ… Rapid matched cost data (numeric-safe)\n",
      "ðŸ’¾ Saved enriched ingredient dataset â†’ D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\ingredients_with_nutrients_cost_env.csv\n",
      "âœ… Final enriched recipes saved â†’ D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\recipes_enriched.csv\n",
      "ðŸ“Š Shape: (20130, 9)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# ðŸ“˜ 01_data_preprocessing.ipynb (Final Optimized Version)\n",
    "# Author: Apoorva Sharma\n",
    "# Project: AI-Powered Precision Nutrition\n",
    "# Purpose: Combine and enrich recipe + ingredient data with USDA, environment & cost\n",
    "# =========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1ï¸âƒ£ Setup paths\n",
    "# ---------------------------------------------------------\n",
    "BASE_DIR = Path(\"D:/Complete_Data/ml_part_nutrition_project\")\n",
    "DATA_DIR = BASE_DIR / \"Datasets\"\n",
    "RECIPE_DIR = DATA_DIR / \"recipe_dataset\"\n",
    "ENV_DIR = DATA_DIR / \"environment_dataset\"\n",
    "COST_DIR = DATA_DIR / \"cost_dataset\"\n",
    "USDA_DIR = DATA_DIR / \"FoodData_Central_sr_legacy_food_csv_2018-04\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed_data\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input paths\n",
    "RECIPES_JSON = RECIPE_DIR / \"full_format_recipes.json\"\n",
    "INGREDIENTS_PATH = RECIPE_DIR / \"ingredients.csv\"\n",
    "USDA_FOOD_PATH = USDA_DIR / \"sr_legacy_food.csv\"\n",
    "USDA_NUTR_PATH = USDA_DIR / \"food_nutrient.csv\"\n",
    "ENV_PATH = ENV_DIR / \"Food_production.csv\"\n",
    "COST_PATH = COST_DIR / \"wfp_food_prices_ind.csv\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2ï¸âƒ£ Load datasets\n",
    "# ---------------------------------------------------------\n",
    "print(\"ðŸ“¥ Loading datasets...\")\n",
    "\n",
    "ingredients = pd.read_csv(INGREDIENTS_PATH)\n",
    "usda_food = pd.read_csv(USDA_FOOD_PATH)\n",
    "usda_nutr = pd.read_csv(USDA_NUTR_PATH)\n",
    "env_data = pd.read_csv(ENV_PATH)\n",
    "cost_data = pd.read_csv(COST_PATH)\n",
    "\n",
    "with open(RECIPES_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    recipes_json = json.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded ingredients: {ingredients.shape}\")\n",
    "print(f\"âœ… Loaded USDA food: {usda_food.shape}\")\n",
    "print(f\"âœ… Loaded USDA nutrient: {usda_nutr.shape}\")\n",
    "print(f\"âœ… Loaded environment: {env_data.shape}\")\n",
    "print(f\"âœ… Loaded cost data: {cost_data.shape}\")\n",
    "print(f\"âœ… Loaded recipes JSON: {len(recipes_json)} recipes\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3ï¸âƒ£ Text Cleaning Utilities\n",
    "# ---------------------------------------------------------\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def rapid_match(word, choices, threshold=0.8):\n",
    "    \"\"\"\n",
    "    A faster approximate matching using substring containment\n",
    "    instead of fuzzywuzzy (much faster for large datasets).\n",
    "    \"\"\"\n",
    "    if not isinstance(word, str) or not word.strip():\n",
    "        return None\n",
    "    for c in choices:\n",
    "        if len(word) > 3 and (word in c or c in word):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# Clean ingredient names\n",
    "ingredients[\"ingredient_clean\"] = ingredients[\"ingredient_raw\"].apply(clean_text)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4ï¸âƒ£ Extract recipe titles & ingredients from JSON (Auto-Detect Format)\n",
    "# ---------------------------------------------------------\n",
    "print(\"ðŸ” Parsing recipe JSON...\")\n",
    "\n",
    "def extract_recipes_auto(recipes_json):\n",
    "    # Case 1: JSON list of dictionaries with 'title' and 'ingredients'\n",
    "    if isinstance(recipes_json, list) and len(recipes_json) > 0:\n",
    "        first_item = recipes_json[0]\n",
    "        \n",
    "        # Case 1a: Deep dict structure (with ingredient text objects)\n",
    "        if isinstance(first_item, dict) and \"ingredients\" in first_item and isinstance(first_item[\"ingredients\"], list):\n",
    "            return pd.DataFrame([\n",
    "                {\n",
    "                    \"recipe_id\": r.get(\"id\", None),\n",
    "                    \"recipe_title\": r.get(\"title\", f\"Recipe_{i}\"),\n",
    "                    \"ingredient_text\": \", \".join([\n",
    "                        clean_text(i.get(\"text\", \"\")) if isinstance(i, dict) else clean_text(str(i))\n",
    "                        for i in r.get(\"ingredients\", [])\n",
    "                    ]),\n",
    "                }\n",
    "                for i, r in enumerate(recipes_json)\n",
    "            ])\n",
    "\n",
    "        # Case 1b: Simplified dict structure with string lists\n",
    "        elif isinstance(first_item, dict) and \"ingredients\" in first_item and isinstance(first_item[\"ingredients\"], list):\n",
    "            return pd.DataFrame([\n",
    "                {\n",
    "                    \"recipe_id\": r.get(\"id\", f\"R_{i}\"),\n",
    "                    \"recipe_title\": r.get(\"title\", f\"Recipe_{i}\"),\n",
    "                    \"ingredient_text\": \", \".join([clean_text(str(x)) for x in r.get(\"ingredients\", [])]),\n",
    "                }\n",
    "                for i, r in enumerate(recipes_json)\n",
    "            ])\n",
    "\n",
    "        # Case 1c: Dictionary without proper keys (flatten)\n",
    "        elif isinstance(first_item, dict):\n",
    "            return pd.DataFrame([\n",
    "                {\n",
    "                    \"recipe_id\": r.get(\"id\", f\"R_{i}\"),\n",
    "                    \"recipe_title\": r.get(\"title\", f\"Recipe_{i}\"),\n",
    "                    \"ingredient_text\": clean_text(str(r)),\n",
    "                }\n",
    "                for i, r in enumerate(recipes_json)\n",
    "            ])\n",
    "\n",
    "        # Case 2: JSON list of strings\n",
    "        elif isinstance(first_item, str):\n",
    "            return pd.DataFrame({\n",
    "                \"recipe_id\": range(len(recipes_json)),\n",
    "                \"recipe_title\": [f\"Recipe_{i}\" for i in range(len(recipes_json))],\n",
    "                \"ingredient_text\": [clean_text(r) for r in recipes_json],\n",
    "            })\n",
    "\n",
    "    # Fallback: return empty DataFrame\n",
    "    return pd.DataFrame(columns=[\"recipe_id\", \"recipe_title\", \"ingredient_text\"])\n",
    "\n",
    "\n",
    "recipes_df = extract_recipes_auto(recipes_json)\n",
    "print(f\"âœ… Extracted recipes: {recipes_df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5ï¸âƒ£ Map USDA nutrient info\n",
    "# ---------------------------------------------------------\n",
    "CORE_NUTRIENTS = {\n",
    "    1008: \"energy_kcal\",\n",
    "    1003: \"protein_g\",\n",
    "    1004: \"fat_g\",\n",
    "    1005: \"carbs_g\",\n",
    "}\n",
    "\n",
    "usda_nutr = usda_nutr[usda_nutr[\"nutrient_id\"].isin(CORE_NUTRIENTS.keys())].copy()\n",
    "usda_nutr[\"nutrient_name\"] = usda_nutr[\"nutrient_id\"].map(CORE_NUTRIENTS)\n",
    "\n",
    "usda_pivot = (\n",
    "    usda_nutr.pivot_table(\n",
    "        index=\"fdc_id\",\n",
    "        columns=\"nutrient_name\",\n",
    "        values=\"amount\",\n",
    "        aggfunc=\"mean\"\n",
    "    ).reset_index()\n",
    ")\n",
    "\n",
    "usda_combined = pd.merge(usda_food, usda_pivot, on=\"fdc_id\", how=\"left\")\n",
    "\n",
    "# Identify description column\n",
    "desc_col = [c for c in usda_combined.columns if \"desc\" in c.lower() or \"food\" in c.lower()]\n",
    "if desc_col:\n",
    "    usda_combined[\"description_clean\"] = usda_combined[desc_col[0]].apply(clean_text)\n",
    "else:\n",
    "    usda_combined[\"description_clean\"] = usda_combined[\"NDB_number\"].astype(str).apply(clean_text)\n",
    "\n",
    "print(\"âœ… USDA combined nutrient table:\", usda_combined.shape)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6ï¸âƒ£ Rapid match ingredients â†’ USDA (nutrition)\n",
    "# ---------------------------------------------------------\n",
    "usda_choices = usda_combined[\"description_clean\"].dropna().unique().tolist()\n",
    "ingredients[\"usda_match\"] = ingredients[\"ingredient_clean\"].apply(\n",
    "    lambda x: rapid_match(x, usda_choices, threshold=0.8)\n",
    ")\n",
    "\n",
    "ingredients = ingredients.merge(\n",
    "    usda_combined,\n",
    "    left_on=\"usda_match\",\n",
    "    right_on=\"description_clean\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(\"âœ… Rapid matched USDA nutrient data\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7ï¸âƒ£ Rapid match environment data\n",
    "# ---------------------------------------------------------\n",
    "env_data[\"Food product_clean\"] = env_data[\"Food product\"].apply(clean_text)\n",
    "env_choices = env_data[\"Food product_clean\"].dropna().unique().tolist()\n",
    "\n",
    "ingredients[\"env_match\"] = ingredients[\"ingredient_clean\"].apply(\n",
    "    lambda x: rapid_match(x, env_choices, threshold=0.8)\n",
    ")\n",
    "\n",
    "ingredients = ingredients.merge(\n",
    "    env_data[[\"Food product_clean\", \"Total_emissions\", \"Land use change\"]],\n",
    "    left_on=\"env_match\",\n",
    "    right_on=\"Food product_clean\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(\"âœ… Rapid matched environmental data\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 8ï¸âƒ£ Clean & match cost data (fixed numeric handling)\n",
    "# ---------------------------------------------------------\n",
    "print(\"ðŸ”¹ Cleaning cost data before averaging...\")\n",
    "\n",
    "cost_data[\"commodity_clean\"] = cost_data[\"commodity\"].apply(clean_text)\n",
    "cost_data[\"usdprice\"] = (\n",
    "    cost_data[\"usdprice\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\",\", \"\", regex=False)\n",
    "    .str.extract(r\"(\\d+\\.?\\d*)\")[0]\n",
    ")\n",
    "cost_data[\"usdprice\"] = pd.to_numeric(cost_data[\"usdprice\"], errors=\"coerce\")\n",
    "cost_data = cost_data.dropna(subset=[\"usdprice\"])\n",
    "\n",
    "avg_cost = cost_data.groupby(\"commodity_clean\", as_index=False)[\"usdprice\"].mean()\n",
    "cost_choices = avg_cost[\"commodity_clean\"].dropna().unique().tolist()\n",
    "\n",
    "ingredients[\"cost_match\"] = ingredients[\"ingredient_clean\"].apply(\n",
    "    lambda x: rapid_match(x, cost_choices, threshold=0.8)\n",
    ")\n",
    "\n",
    "ingredients = ingredients.merge(\n",
    "    avg_cost, left_on=\"cost_match\", right_on=\"commodity_clean\", how=\"left\"\n",
    ")\n",
    "ingredients.rename(columns={\"usdprice\": \"price\"}, inplace=True)\n",
    "print(\"âœ… Rapid matched cost data (numeric-safe)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 9ï¸âƒ£ Save combined ingredient-level dataset\n",
    "# ---------------------------------------------------------\n",
    "ingredient_save_path = PROCESSED_DIR / \"ingredients_with_nutrients_cost_env.csv\"\n",
    "ingredients.to_csv(ingredient_save_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved enriched ingredient dataset â†’ {ingredient_save_path}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ”Ÿ Aggregate recipe-level nutrients\n",
    "# ---------------------------------------------------------\n",
    "nutr_cols = [\"energy_kcal\", \"protein_g\", \"fat_g\", \"carbs_g\", \"price\", \"Total_emissions\"]\n",
    "nutr_cols = [c for c in nutr_cols if c in ingredients.columns]\n",
    "\n",
    "recipe_summary = (\n",
    "    ingredients.groupby(\"recipe_title\")[nutr_cols]\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={c: f\"{c}_mean\" for c in nutr_cols})\n",
    ")\n",
    "\n",
    "recipes_enriched = pd.merge(recipes_df, recipe_summary, on=\"recipe_title\", how=\"left\")\n",
    "recipes_enriched.to_csv(PROCESSED_DIR / \"recipes_enriched.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Final enriched recipes saved â†’ {PROCESSED_DIR / 'recipes_enriched.csv'}\")\n",
    "print(f\"ðŸ“Š Shape: {recipes_enriched.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44564f3-9b81-4aae-9cf8-1e62b760c3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ingredients from: D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\ingredients_with_nutrients_cost_env.csv\n",
      "Loaded ingredients: (199106, 19)\n",
      "Columns: ['recipe_id', 'recipe_title', 'ingredient_raw', 'ingredient_clean', 'usda_match', 'fdc_id', 'NDB_number', 'carbs_g', 'energy_kcal', 'fat_g', 'protein_g', 'description_clean', 'env_match', 'Food product_clean', 'Total_emissions', 'Land use change', 'cost_match', 'commodity_clean', 'price']\n",
      "Loading environment data from: D:\\Complete_Data\\ml_part_nutrition_project\\Datasets\\environment_dataset\\Food_production.csv\n",
      "Loaded environment: (43, 23)\n",
      "Environment columns: ['Food product', 'Land use change', 'Animal Feed', 'Farm', 'Processing', 'Transport', 'Packging', 'Retail', 'Total_emissions', 'Eutrophying emissions per 1000kcal (gPOâ‚„eq per 1000kcal)', 'Eutrophying emissions per kilogram (gPOâ‚„eq per kilogram)', 'Eutrophying emissions per 100g protein (gPOâ‚„eq per 100 grams protein)', 'Freshwater withdrawals per 1000kcal (liters per 1000kcal)', 'Freshwater withdrawals per 100g protein (liters per 100g protein)', 'Freshwater withdrawals per kilogram (liters per kilogram)', 'Greenhouse gas emissions per 1000kcal (kgCOâ‚‚eq per 1000kcal)', 'Greenhouse gas emissions per 100g protein (kgCOâ‚‚eq per 100g protein)', 'Land use per 1000kcal (mÂ² per 1000kcal)', 'Land use per kilogram (mÂ² per kilogram)', 'Land use per 100g protein (mÂ² per 100g protein)', 'Scarcity-weighted water use per kilogram (liters per kilogram)', 'Scarcity-weighted water use per 100g protein (liters per 100g protein)', 'Scarcity-weighted water use per 1000kcal (liters per 1000 kilocalories)']\n",
      "Detected environment-like columns (candidates):\n",
      "  - Land use change\n",
      "  - Total_emissions\n",
      "  - Eutrophying emissions per 1000kcal (gPOâ‚„eq per 1000kcal)\n",
      "  - Eutrophying emissions per kilogram (gPOâ‚„eq per kilogram)\n",
      "  - Eutrophying emissions per 100g protein (gPOâ‚„eq per 100 grams protein)\n",
      "  - Freshwater withdrawals per 1000kcal (liters per 1000kcal)\n",
      "  - Freshwater withdrawals per 100g protein (liters per 100g protein)\n",
      "  - Freshwater withdrawals per kilogram (liters per kilogram)\n",
      "  - Greenhouse gas emissions per 1000kcal (kgCOâ‚‚eq per 1000kcal)\n",
      "  - Greenhouse gas emissions per 100g protein (kgCOâ‚‚eq per 100g protein)\n",
      "  - Land use per 1000kcal (mÂ² per 1000kcal)\n",
      "  - Land use per kilogram (mÂ² per kilogram)\n",
      "  - Land use per 100g protein (mÂ² per 100g protein)\n",
      "  - Scarcity-weighted water use per kilogram (liters per kilogram)\n",
      "  - Scarcity-weighted water use per 100g protein (liters per 100g protein)\n",
      "  - Scarcity-weighted water use per 1000kcal (liters per 1000 kilocalories)\n",
      "Selected emission column: Total_emissions\n",
      "Selected land column: Land use change\n",
      "Using environment name column: Food product\n",
      "Unique env choices: 43\n",
      "Using ingredient column: ingredient_raw\n",
      "Mapping ingredients to environment rows (batched)...\n",
      "Unique ingredients count: 81749\n",
      "  matched 5000/81749 unique ingredients...\n",
      "  matched 10000/81749 unique ingredients...\n",
      "  matched 20000/81749 unique ingredients...\n",
      "  matched 25000/81749 unique ingredients...\n",
      "  matched 30000/81749 unique ingredients...\n",
      "  matched 35000/81749 unique ingredients...\n",
      "  matched 40000/81749 unique ingredients...\n",
      "  matched 45000/81749 unique ingredients...\n",
      "  matched 50000/81749 unique ingredients...\n",
      "  matched 55000/81749 unique ingredients...\n",
      "  matched 60000/81749 unique ingredients...\n",
      "  matched 65000/81749 unique ingredients...\n",
      "  matched 70000/81749 unique ingredients...\n",
      "  matched 75000/81749 unique ingredients...\n",
      "  matched 80000/81749 unique ingredients...\n",
      "Mapping build done in 6.2s\n",
      "Saved ingredient-level env mapping to: D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\ingredients_with_env_mapped.csv\n",
      "Ingredients shape now: (199106, 22)\n",
      "Saved recipe-level env metrics to: D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\recipes_with_env_metrics.csv\n",
      "Recipe-agg shape: (20130, 3)\n",
      "\n",
      "Done. Summary:\n",
      "Ingredients file: D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\ingredients_with_env_mapped.csv\n",
      "Recipe metrics file: D:\\Complete_Data\\ml_part_nutrition_project\\processed_data\\recipes_with_env_metrics.csv\n",
      "Time elapsed: 21.9s\n"
     ]
    }
   ],
   "source": [
    "# 02_environment_mapping_fast.ipynb\n",
    "# Purpose: map environmental metrics (emissions, land use, water, ...) to ingredients,\n",
    "# and aggregate to recipe-level. Uses difflib for fast approximate matching.\n",
    "# Author: adapted for Apoorva Sharma project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import difflib\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# 0. Paths & config (edit if needed)\n",
    "# ----------------------------\n",
    "BASE_DIR = Path(\"D:/Complete_Data/ml_part_nutrition_project\")\n",
    "DATASETS_DIR = BASE_DIR / \"Datasets\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed_data\"\n",
    "ENV_DIR = DATASETS_DIR / \"environment_dataset\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ingredient-level file produced earlier by preprocessing step\n",
    "INGREDIENTS_PATH_CANDIDATES = [\n",
    "    PROCESSED_DIR / \"ingredients_with_nutrients_cost_env.csv\",\n",
    "    PROCESSED_DIR / \"ingredients_with_nutrients.csv\",\n",
    "    BASE_DIR / \"processed_data\" / \"ingredients_with_nutrients_cost_env.csv\"\n",
    "]\n",
    "\n",
    "# Environment data file (your Food_Production / environment dataset)\n",
    "ENV_CANDIDATES = [\n",
    "    ENV_DIR / \"Food_production.csv\",\n",
    "    ENV_DIR / \"Food_Production.csv\",\n",
    "    DATASETS_DIR / \"environment_dataset\" / \"Food_Production.csv\",\n",
    "    DATASETS_DIR / \"environment_dataset\" / \"Food_production.csv\"\n",
    "]\n",
    "\n",
    "# Output filenames\n",
    "OUT_ING = PROCESSED_DIR / \"ingredients_with_env_mapped.csv\"\n",
    "OUT_REC = PROCESSED_DIR / \"recipes_with_env_metrics.csv\"\n",
    "\n",
    "# similarity threshold for difflib.get_close_matches (0..1 scaled later)\n",
    "MATCH_CUTOFF = 0.70   # conservative; adjust 0.6..0.8 as needed\n",
    "MAX_CANDIDATES = 5    # how many close matches to examine\n",
    "\n",
    "# Batch size for applying mapping to avoid memory spikes\n",
    "BATCH_SIZE = 20000\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Helper functions\n",
    "# ----------------------------\n",
    "def normalize_text(s):\n",
    "    \"\"\"Lowercase, strip punctuation and extra spaces.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def find_best_match(name, choices_list, cutoff=MATCH_CUTOFF):\n",
    "    \"\"\"\n",
    "    Use difflib.get_close_matches to find best approximate match.\n",
    "    Returns matched choice string or None.\n",
    "    \"\"\"\n",
    "    if not name or not isinstance(name, str):\n",
    "        return None\n",
    "    # difflib works on raw strings; normalize externally\n",
    "    candidates = difflib.get_close_matches(name, choices_list, n=MAX_CANDIDATES, cutoff=cutoff)\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load files (robust selection)\n",
    "# ----------------------------\n",
    "# Ingredients file\n",
    "INGREDIENTS_PATH = None\n",
    "for p in INGREDIENTS_PATH_CANDIDATES:\n",
    "    if p.exists():\n",
    "        INGREDIENTS_PATH = p\n",
    "        break\n",
    "if INGREDIENTS_PATH is None:\n",
    "    raise FileNotFoundError(f\"Could not find ingredients file. Looked at: {INGREDIENTS_PATH_CANDIDATES}\")\n",
    "\n",
    "print(\"Loading ingredients from:\", INGREDIENTS_PATH)\n",
    "ingredients = pd.read_csv(INGREDIENTS_PATH, low_memory=False)\n",
    "print(\"Loaded ingredients:\", ingredients.shape)\n",
    "print(\"Columns:\", list(ingredients.columns)[:40])\n",
    "\n",
    "# Environment file\n",
    "ENV_PATH = None\n",
    "for p in ENV_CANDIDATES:\n",
    "    if p.exists():\n",
    "        ENV_PATH = p\n",
    "        break\n",
    "if ENV_PATH is None:\n",
    "    raise FileNotFoundError(f\"Could not find environment file. Looked at: {ENV_CANDIDATES}\")\n",
    "\n",
    "print(\"Loading environment data from:\", ENV_PATH)\n",
    "env_df = pd.read_csv(ENV_PATH, low_memory=False)\n",
    "print(\"Loaded environment:\", env_df.shape)\n",
    "print(\"Environment columns:\", list(env_df.columns)[:60])\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Identify useful env metric columns\n",
    "# ----------------------------\n",
    "env_col_candidates = [c for c in env_df.columns if any(k in c.lower() for k in (\n",
    "    \"emission\", \"total_emissions\", \"total_emission\", \"greenhouse\", \"ghg\", \"co2\",\n",
    "    \"land\", \"land use\", \"landuse\", \"land_use\", \"land per\", \"eutroph\", \"water\", \"freshwater\"))]\n",
    "\n",
    "# Also show a short list to the user for verification\n",
    "print(\"Detected environment-like columns (candidates):\")\n",
    "for c in env_col_candidates[:40]:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "# Choose the most likely ones\n",
    "preferred_emission_cols = [c for c in env_col_candidates if \"emiss\" in c.lower() or \"total_emissions\" in c.lower() or \"greenhouse\" in c.lower()]\n",
    "preferred_land_cols = [c for c in env_col_candidates if \"land\" in c.lower()]\n",
    "\n",
    "emission_col = preferred_emission_cols[0] if preferred_emission_cols else (env_col_candidates[0] if env_col_candidates else None)\n",
    "land_col = preferred_land_cols[0] if preferred_land_cols else None\n",
    "\n",
    "print(\"Selected emission column:\", emission_col)\n",
    "print(\"Selected land column:\", land_col)\n",
    "\n",
    "# If none found, list some manual column names for user to update\n",
    "if emission_col is None:\n",
    "    raise ValueError(\"No emission-like column detected in environment dataset. Please inspect `env_df.columns` and update the script.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Prepare matching keys\n",
    "# ----------------------------\n",
    "# find plausable product name column in env_df\n",
    "possible_name_cols = [c for c in env_df.columns if any(k in c.lower() for k in (\"food product\",\"food\",\"product\",\"commodity\",\"item\",\"name\"))]\n",
    "env_name_col = possible_name_cols[0] if possible_name_cols else env_df.columns[0]\n",
    "print(\"Using environment name column:\", env_name_col)\n",
    "\n",
    "# create cleaned name column in environment dataset\n",
    "env_df[\"env_name_clean\"] = env_df[env_name_col].astype(str).apply(normalize_text)\n",
    "\n",
    "# create choices list (unique)\n",
    "env_choices = env_df[\"env_name_clean\"].dropna().unique().tolist()\n",
    "print(f\"Unique env choices: {len(env_choices)}\")\n",
    "\n",
    "# prepare ingredient names to match\n",
    "# find a good ingredient text column in ingredients file\n",
    "possible_ing_cols = [c for c in ingredients.columns if any(k in c.lower() for k in (\"ingredient_clean\",\"ingredient_text\",\"ingredient_raw\",\"ingredient\"))]\n",
    "if possible_ing_cols:\n",
    "    ing_col = possible_ing_cols[0]\n",
    "else:\n",
    "    # fallback - pick the first column that looks like string\n",
    "    ing_col = ingredients.columns[0]\n",
    "\n",
    "print(\"Using ingredient column:\", ing_col)\n",
    "\n",
    "# ensure we have a cleaned ingredient column\n",
    "if \"ingredient_clean\" not in ingredients.columns:\n",
    "    ingredients[\"ingredient_clean\"] = ingredients[ing_col].astype(str).apply(normalize_text)\n",
    "else:\n",
    "    # normalize existing column\n",
    "    ingredients[\"ingredient_clean\"] = ingredients[\"ingredient_clean\"].astype(str).apply(normalize_text)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Rapid mapping (batched) from ingredient -> env record\n",
    "# ----------------------------\n",
    "start_time = time.time()\n",
    "print(\"Mapping ingredients to environment rows (batched)...\")\n",
    "\n",
    "# Pre-memoize mapping for unique ingredient strings to avoid repeated matching\n",
    "unique_ings = ingredients[\"ingredient_clean\"].dropna().unique().tolist()\n",
    "print(\"Unique ingredients count:\", len(unique_ings))\n",
    "\n",
    "# build a dict of mapping for unique names using difflib\n",
    "mapping = {}\n",
    "for i, name in enumerate(unique_ings):\n",
    "    # quick heuristic: if name appears in any env choice as substring, prefer that\n",
    "    if not name:\n",
    "        mapping[name] = None\n",
    "        continue\n",
    "\n",
    "    # substring direct match\n",
    "    substr_matches = [c for c in env_choices if name in c or c in name]\n",
    "    if substr_matches:\n",
    "        mapping[name] = substr_matches[0]\n",
    "        continue\n",
    "\n",
    "    # difflib approximate match\n",
    "    best = find_best_match(name, env_choices, cutoff=MATCH_CUTOFF)\n",
    "    mapping[name] = best\n",
    "\n",
    "    # progress print occasionally\n",
    "    if (i+1) % 5000 == 0:\n",
    "        print(f\"  matched {i+1}/{len(unique_ings)} unique ingredients...\")\n",
    "\n",
    "print(\"Mapping build done in {:.1f}s\".format(time.time() - start_time))\n",
    "\n",
    "# apply mapping to ingredients in batches (to avoid huge intermediate arrays)\n",
    "mapped_env_names = []\n",
    "for start in range(0, len(ingredients), BATCH_SIZE):\n",
    "    batch = ingredients[\"ingredient_clean\"].iloc[start:start+BATCH_SIZE].tolist()\n",
    "    mapped_batch = [mapping.get(x, None) for x in batch]\n",
    "    mapped_env_names.extend(mapped_batch)\n",
    "ingredients[\"env_match\"] = mapped_env_names\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Join selected environment metrics\n",
    "# ----------------------------\n",
    "# choose subset of env_df to merge\n",
    "env_subset = env_df[[\"env_name_clean\", emission_col] + ([land_col] if land_col else [])].drop_duplicates(subset=[\"env_name_clean\"])\n",
    "# ensure numeric\n",
    "env_subset[emission_col] = pd.to_numeric(env_subset[emission_col], errors=\"coerce\")\n",
    "if land_col:\n",
    "    env_subset[land_col] = pd.to_numeric(env_subset[land_col], errors=\"coerce\")\n",
    "\n",
    "ingredients = ingredients.merge(env_subset, left_on=\"env_match\", right_on=\"env_name_clean\", how=\"left\", suffixes=(\"\", \"_env\"))\n",
    "\n",
    "# rename to unified column names\n",
    "ingredients = ingredients.rename(columns={emission_col: \"Total_emissions\", land_col: \"Land_use_change\"} if land_col else {emission_col: \"Total_emissions\"})\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Fallbacks / cleaning\n",
    "# ----------------------------\n",
    "# If some ingredients missing emissions, fill with median of mapped values\n",
    "if \"Total_emissions\" in ingredients.columns:\n",
    "    median_em = ingredients[\"Total_emissions\"].median(skipna=True)\n",
    "    ingredients[\"Total_emissions\"] = ingredients[\"Total_emissions\"].fillna(median_em)\n",
    "else:\n",
    "    ingredients[\"Total_emissions\"] = np.nan\n",
    "\n",
    "if \"Land_use_change\" in ingredients.columns:\n",
    "    median_land = ingredients[\"Land_use_change\"].median(skipna=True)\n",
    "    ingredients[\"Land_use_change\"] = ingredients[\"Land_use_change\"].fillna(median_land)\n",
    "\n",
    "# Save ingredient-level mapping\n",
    "ingredients.to_csv(OUT_ING, index=False)\n",
    "print(\"Saved ingredient-level env mapping to:\", OUT_ING)\n",
    "print(\"Ingredients shape now:\", ingredients.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Aggregate per recipe\n",
    "# ----------------------------\n",
    "# Determine the recipe title/key column in ingredients. Try common names.\n",
    "possible_recipe_cols = [c for c in ingredients.columns if any(k in c.lower() for k in (\"recipe_title\",\"recipe_id\",\"title\",\"recipe\"))]\n",
    "recipe_key_col = possible_recipe_cols[0] if possible_recipe_cols else None\n",
    "\n",
    "if recipe_key_col is None:\n",
    "    # Try to join with recipes file in processed_data if exists\n",
    "    print(\"No recipe_key in ingredients â€” attempting to find recipes file to map by index/title...\")\n",
    "    recipe_candidates = [\n",
    "        PROCESSED_DIR / \"recipes_enriched.csv\",\n",
    "        PROCESSED_DIR / \"recipes_master.csv\",\n",
    "        PROCESSED_DIR / \"recipes_enriched_v2.csv\"\n",
    "    ]\n",
    "    recipes_df = None\n",
    "    for p in recipe_candidates:\n",
    "        if p.exists():\n",
    "            recipes_df = pd.read_csv(p, low_memory=False)\n",
    "            print(\"Loaded recipes file for aggregation:\", p)\n",
    "            break\n",
    "    if recipes_df is None:\n",
    "        # can't aggregate by recipe; simply compute ingredient-level metrics and finish\n",
    "        print(\"Could not find recipes file; skipping recipe aggregation. You can aggregate externally.\")\n",
    "        recipes_agg = pd.DataFrame()  # empty\n",
    "    else:\n",
    "        # try to merge by a likely field (title vs recipe_title)\n",
    "        if \"title\" in recipes_df.columns:\n",
    "            # build mapping from recipe title -> ingredients (if ingredients has info to join)\n",
    "            # if ingredients had a recipe_title column earlier it'll be used; else we can't map reliably\n",
    "            if \"recipe_title\" in ingredients.columns:\n",
    "                recipes_agg = ingredients.groupby(\"recipe_title\")[[\"Total_emissions\", \"Land_use_change\"]].mean().reset_index()\n",
    "            else:\n",
    "                # attempt to derive mapping from ingredients file: see if ingredients has 'recipe_id' or other id columns\n",
    "                possible_id_cols = [c for c in ingredients.columns if \"id\" in c.lower()]\n",
    "                if possible_id_cols:\n",
    "                    idcol = possible_id_cols[0]\n",
    "                    recipes_agg = ingredients.groupby(idcol)[[\"Total_emissions\", \"Land_use_change\"]].mean().reset_index().rename(columns={idcol: \"recipe_id\"})\n",
    "                else:\n",
    "                    print(\"No recipe linking column found inside ingredients. Skipping recipe aggregation.\")\n",
    "                    recipes_agg = pd.DataFrame()\n",
    "else:\n",
    "    # standard path: aggregate grouped by recipe key present in ingredients\n",
    "    recipes_agg = ingredients.groupby(recipe_key_col)[[\"Total_emissions\", \"Land_use_change\"]].mean().reset_index()\n",
    "    recipes_agg = recipes_agg.rename(columns={recipe_key_col: \"recipe_title\"})\n",
    "\n",
    "# Save recipe-level aggregation if not empty\n",
    "if not recipes_agg.empty:\n",
    "    recipes_agg.to_csv(OUT_REC, index=False)\n",
    "    print(\"Saved recipe-level env metrics to:\", OUT_REC)\n",
    "    print(\"Recipe-agg shape:\", recipes_agg.shape)\n",
    "else:\n",
    "    print(\"Recipe aggregation skipped or no mapping available. You still have ingredient-level file:\", OUT_ING)\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Summary\n",
    "# ----------------------------\n",
    "print(\"\\nDone. Summary:\")\n",
    "print(\"Ingredients file:\", OUT_ING)\n",
    "if not recipes_agg.empty:\n",
    "    print(\"Recipe metrics file:\", OUT_REC)\n",
    "print(\"Time elapsed: {:.1f}s\".format(time.time() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (new)",
   "language": "python",
   "name": "python3-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
